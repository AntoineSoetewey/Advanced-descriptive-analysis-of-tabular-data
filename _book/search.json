[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Descriptive Analysis of Tabular Data",
    "section": "",
    "text": "Preface\nDescriptive statistics are often treated as a preliminary step—a routine box to check before moving on to inference, prediction, or causal analysis. Yet in practice, understanding the structure, associations, and patterns within complex tabular data is neither trivial nor purely mechanical. It requires sophisticated methods, thoughtful visualization, and clear communication.\nThis book synthesizes advanced techniques for descriptive analysis of tabular data, drawing on recent developments in machine learning, network analysis, and interactive visualization. Our goal is to equip researchers, analysts, policymakers, and data journalists with tools that go beyond means and standard deviations, enabling them to extract actionable insights from multivariate datasets.\nThe methods and tools presented here are not a repackaging of standard EDA. They reflect original methodological syntheses, implementation choices, and applied workflows developed through research and field practice. In that sense, this book also serves as a portfolio: a concrete, citable body of work that demonstrates innovation in descriptive analytics and foregrounds substantive methodological originality.\nThe material presented here emerged from postdoctoral research at the intersection of applied statistics, machine learning, and data visualization. It reflects a pragmatic philosophy: methods should be interpretable, visual, and suitable for communicating findings to statistically literate but non-technical audiences.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#who-this-book-is-for",
    "href": "index.html#who-this-book-is-for",
    "title": "Advanced Descriptive Analysis of Tabular Data",
    "section": "Who This Book Is For",
    "text": "Who This Book Is For\nThis book is designed for readers who already possess a solid foundation in statistics—including regression analysis, hypothesis testing, and basic multivariate methods. We assume familiarity with concepts like correlation, variance decomposition, and model evaluation.\nOur intended audience includes:\n\nResearchers and applied scientists seeking exploratory tools for complex datasets\nPolicymakers and analysts in government and public institutions\nData journalists investigating patterns in social, economic, or health data\nConsultants and analytical teams in private firms\nGraduate students in statistics, data science, public policy, or related fields\n\nThe material is suitable for a Master-level university course and may serve as a foundation for doctoral-level methodological training.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#philosophy-and-approach",
    "href": "index.html#philosophy-and-approach",
    "title": "Advanced Descriptive Analysis of Tabular Data",
    "section": "Philosophy and Approach",
    "text": "Philosophy and Approach\nThe unifying thread throughout this book is: How do we move beyond standard descriptive statistics to extract, visualize, and communicate structure in complex tabular data?\nWe emphasize:\n\nInterpretability: Methods that produce understandable results\nVisual analytics: Graphs and interactive tools as primary analytical instruments\nMethodological transparency: Explicit discussion of assumptions, trade-offs, and limitations\nCommunication: Presenting results to diverse audiences, from technical peers to policy stakeholders\n\nRather than purely theoretical exposition, we ground each method in real applied use cases, demonstrating how techniques perform on actual data challenges.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#structure-of-the-book",
    "href": "index.html#structure-of-the-book",
    "title": "Advanced Descriptive Analysis of Tabular Data",
    "section": "Structure of the Book",
    "text": "Structure of the Book\nThe book is organized into seven parts:\nPart I: Foundations establishes the conceptual framework, revisiting what it means to “describe” data and introducing the challenge of mixed-type variables and multivariate associations.\nPart II focuses on association analysis—measuring relationships between pairs of variables of different types and representing these associations as networks.\nPart III introduces interactive visual analytics, including the AssociationExplorer Shiny application, which operationalizes association-focused methods in a unified exploratory interface.\nParts IV–VI present three families of advanced methods for higher-dimensional structure: tree-based models for segmentation and description, interpretable machine learning techniques for understanding complex patterns, and AutoML approaches that automate exploration.\nPart VII presents extended applied case studies from policy analysis, public health, and business analytics, demonstrating how these methods solve real-world problems.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Advanced Descriptive Analysis of Tabular Data",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis work has been shaped by collaborations with colleagues, conversations with practitioners, and feedback from students and the open-source community. We are particularly grateful to researchers from UCLouvain Saint-Louis Brussels involved in the Beamm research project for their insights and support.\nWe also acknowledge the open-source software communities whose tools make this work possible, including R, Python, Shiny, Quarto, and countless contributed packages.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "Advanced Descriptive Analysis of Tabular Data",
    "section": "How to Use This Book",
    "text": "How to Use This Book\nChapters can be read sequentially or selectively, depending on your background and interests. Readers already familiar with tree-based methods might skip Part II; those primarily interested in association analysis could jump directly to Part V.\nThroughout the book, we provide code examples in R and references to accompanying interactive tools. All datasets, code and source files are available on GitHub.\nWe encourage readers to experiment with the methods on their own data. Descriptive analysis is learned through practice, and the best way to internalize these techniques is to apply them to problems you care about.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About the authors",
    "section": "",
    "text": "Antoine Soetewey is a postdoctoral researcher in data science and statistics at HEC Liège and UCLouvain Saint-Louis Brussels. He works on statistical methods and data analysis, with a focus on clear communication and practical applications. More information is available at antoinesoetewey.com.\nCédric Heuchenne is a professor at UCLouvain and HEC Liège. He is affiliated with research and teaching units in data analysis and modeling, and contributes expertise in statistics and applied methods.",
    "crumbs": [
      "About the authors"
    ]
  },
  {
    "objectID": "chapter-00-introduction.html",
    "href": "chapter-00-introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 The Challenge of Describing Complex Data\nWhen confronted with a dataset containing dozens or hundreds of variables—mixing continuous measurements, categorical indicators, ordinal scales, and binary flags—how do we make sense of it? How do we identify the most important patterns, detect unexpected associations, and communicate our findings effectively?\nTraditional descriptive statistics (means, medians, frequency tables, correlation matrices) remain essential, but they quickly become insufficient as data complexity grows. A correlation matrix for 50 variables contains 1,225 pairwise relationships, most of which are noise. Standard summary statistics provide no guidance on which variables matter most, how they interact, or what segments or subpopulations might exist in the data.\nThis book addresses the challenge of advanced descriptive analysis: moving beyond elementary summaries to extract meaningful structure from complex tabular data. It is intentionally positioned as an original methodological portfolio—combining novel syntheses, practical tooling, and reproducible workflows that evidence substantive methodological originality.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter-00-introduction.html#what-is-descriptive-analysis",
    "href": "chapter-00-introduction.html#what-is-descriptive-analysis",
    "title": "1  Introduction",
    "section": "1.2 What Is Descriptive Analysis?",
    "text": "1.2 What Is Descriptive Analysis?\nDescriptive analysis characterizes the properties of a dataset—its distributions, central tendencies, variability, associations, and patterns—without making claims about causation or population-level inference. While often contrasted with inferential or predictive analysis, descriptive work is neither simpler nor less valuable.\nGood descriptive analysis:\n\nReveals structure: identifying clusters, segments, or natural groupings\nQuantifies associations: measuring relationships between variables of mixed types\nGuides further analysis: highlighting variables and relationships worthy of deeper investigation\nCommunicates findings: translating complex patterns into actionable insights\n\nIn many applied contexts—policy evaluation, exploratory journalism, early-stage research—descriptive analysis is the primary goal, not a mere prelude to inference.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter-00-introduction.html#why-advanced-methods",
    "href": "chapter-00-introduction.html#why-advanced-methods",
    "title": "1  Introduction",
    "section": "1.3 Why Advanced Methods?",
    "text": "1.3 Why Advanced Methods?\nStandard descriptive tools have well-known limitations:\n\nUnivariate summaries ignore multivariate structure and conditional relationships\nCorrelation coefficients only capture linear associations between continuous variables\nCross-tabulations become unwieldy with many categories or variables\nScatterplot matrices fail to scale beyond a handful of variables\n\nAdvanced methods address these limitations by:\n\nHandling mixed-type variables: combining continuous, categorical, ordinal, and binary data in a unified framework\nCapturing nonlinear relationships: detecting patterns that correlation coefficients miss\nAutomating discovery: using algorithmic approaches to identify important features and interactions\nVisualizing high-dimensional structure: representing complex associations through networks, trees, and interactive graphics\nEnabling exploration: providing interactive tools that allow analysts to interrogate data from multiple angles",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter-00-introduction.html#methods-covered-in-this-book",
    "href": "chapter-00-introduction.html#methods-covered-in-this-book",
    "title": "1  Introduction",
    "section": "1.4 Methods Covered in This Book",
    "text": "1.4 Methods Covered in This Book\nThis book synthesizes several methodological traditions:\n\n1.4.1 Tree-Based Methods\nRegression and classification trees offer a powerful yet interpretable approach to segmenting populations and understanding conditional structure in data. Trees recursively partition data based on variable thresholds, producing interpretable decision rules that separate observations into relatively homogeneous groups. Unlike black-box predictive models, tree structures are transparent: practitioners can easily explain why a particular observation falls into a specific segment. Trees naturally reveal which variables are most discriminative and at what thresholds decisions change. This makes them invaluable for exploratory work, program targeting, and communicating findings to stakeholders who value transparency. Moreover, ensemble extensions—combining multiple trees through random forests or boosting—improve robustness while preserving the ability to extract interpretable variable importance measures and identify complex interactions.\n\n\n1.4.2 Interpretable Machine Learning\nModern machine learning models often achieve superior predictive accuracy compared to classical statistical methods, but at the cost of interpretability. Interpretable machine learning bridges this gap by providing techniques to understand what complex models have learned from data. Methods like permutation-based feature importance identify which variables the model relies on most heavily. Individual conditional expectation curves visualize how predictions change as individual features vary, revealing nonlinear relationships and thresholds. Shapley values—grounded in cooperative game theory—decompose each prediction into additive contributions from each feature, providing both global importance rankings and local explanations for individual observations. These post-hoc interpretation tools transform predictive models into descriptive instruments, enabling analysts to extract actionable insights about variable relationships while leveraging the flexibility of modern ML algorithms.\n\n\n1.4.3 AutoML for Exploration\nAutomated machine learning platforms systematically search across hundreds or thousands of model configurations, feature transformations, and hyperparameters to identify the best-performing models for a given task. Rather than viewing AutoML purely as a prediction tool, we leverage it as an exploratory instrument. AutoML workflows automatically discover which features matter, which transformations improve predictive signals, and which variable interactions are important. By screening a vast model space, AutoML can identify complex patterns that might be missed through manual feature engineering or simpler methods. When interpreted descriptively—focusing on which transformations boost performance rather than out-of-sample accuracy itself—AutoML becomes a rapid hypothesis-generation engine, especially valuable for preliminary analysis of new datasets or when domain expertise is limited. The rankings and performances of different models also reveal which features and interactions the data best supports.\n\n\n1.4.4 Association Measures\nA central challenge in descriptive analysis is measuring association when variables are not all continuous. Real-world datasets routinely mix quantitative, qualitative, ordinal, and binary variables, making classical measures like Pearson correlation inadequate or misleading. This book presents a type-aware framework for association that selects and scales association measures according to the specific combination of variable types being related. For continuous-continuous pairs, we discuss Pearson, Spearman, and distance-based correlations. For categorical-categorical associations, we cover Cramér’s V and related measures. For mixed pairs, we employ model-based measures and mutual information approaches. Crucially, these measures are interpreted descriptively rather than inferentially: the goal is not null hypothesis testing, but comparability and ranking—identifying which variable pairs exhibit relatively strong relationships meriting closer inspection. By placing heterogeneous associations on a common scale (often \\([0, 1]\\)), analysts can scan large multivariate datasets and focus attention on the most informative relationships, regardless of variable type. This pragmatic, unified treatment transforms a fragmented set of statistical tools into a coherent exploratory framework.\n\n\n1.4.5 Network Representations\nWhen a dataset contains dozens or hundreds of variables, even a well-chosen association measure produces an overwhelming matrix of pairwise relationships. Network-based representations address this scalability challenge by encoding associations as variable networks where nodes represent variables and edges represent relationships exceeding a chosen threshold. Edge weights or colors encode association magnitudes, while network layouts position variables spatially such that strongly related variables cluster near each other. This spatial organization renders high-dimensional association structure visible and interpretable at a glance. Beyond individual associations, network analysis reveals global structure: communities of tightly interconnected variables that may represent distinct domains or latent constructs, hub variables that bridge multiple domains, and peripheral variables carrying unique information. Centrality measures (degree, betweenness, eigenvector) identify influential variables. Community detection algorithms partition variables into meaningful groups. These higher-order network properties are difficult to discern from association matrices or pairwise plots alone, yet they often provide crucial insights into data structure. Network representations therefore serve as a cognitive map of the dataset, guiding exploratory analysis through high-dimensional association space.\n\n\n1.4.6 Interactive Visual Analytics\nStatic visualizations answer pre-determined questions; interactive tools enable dynamic exploration. Interactive graphics—particularly Shiny applications—allow analysts to filter data by conditions, aggregate across subgroups, adjust plot parameters, and link multiple views in real time. This interactivity supports hypothesis generation and refinement: analysts can test “what if” scenarios, drill down into subpopulations, and detect patterns that might not appear in static plots. Dashboards combine multiple interactive visualizations into coordinated workflows, allowing stakeholders to explore data according to their own questions rather than passively receiving predetermined findings. For descriptive analysis in particular, interactivity is essential for handling high-dimensional data. An interactive tool can present association networks, tree structures, and distributions while allowing users to focus on subsets, time periods, or demographic groups of interest. This chapter introduces Shiny-based applications and demonstrates how to build interactive descriptive tools that scale to real-world data complexity.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter-00-introduction.html#real-world-applications",
    "href": "chapter-00-introduction.html#real-world-applications",
    "title": "1  Introduction",
    "section": "1.5 Real-World Applications",
    "text": "1.5 Real-World Applications\nEach method is illustrated with applied examples drawn from:\n\nPublic policy: understanding determinants of program participation, analyzing survey data on citizen attitudes\nPublic health: exploring risk factors in epidemiological data, characterizing patient populations\nBusiness analytics: segmenting customers, identifying drivers of satisfaction or churn\nSocial science research: analyzing survey responses, detecting patterns in observational data\nData journalism: investigating patterns in government data, economic indicators, or social trends\n\nThroughout the book, these methods are implemented in R and demonstrated on real datasets. A recurring example is the AssociationExplorer Shiny application, developed as part of the research underlying this work, which integrates multiple descriptive techniques into a unified interactive interface. While all methods can be implemented using standard statistical software, AssociationExplorer provides a practical tool for immediate exploratory use. These examples demonstrate that advanced descriptive methods are not academic exercises—they solve genuine problems faced by analysts across diverse fields and showcase original contributions.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter-00-introduction.html#relationship-to-other-analytical-goals",
    "href": "chapter-00-introduction.html#relationship-to-other-analytical-goals",
    "title": "1  Introduction",
    "section": "1.6 Relationship to Other Analytical Goals",
    "text": "1.6 Relationship to Other Analytical Goals\nDescriptive analysis intersects with but differs from:\n\nExploratory Data Analysis (EDA): Descriptive analysis is a form of EDA, but emphasizes quantitative measures and formal methods alongside graphical exploration\nPredictive modeling: We use predictive models descriptively, focusing on interpretation rather than out-of-sample performance\nCausal inference: Descriptive analysis identifies associations but does not claim causation; it can, however, inform causal hypotheses\nDimension reduction: Methods like PCA and MCA reduce dimensionality; we emphasize interpretable summaries that preserve variable identities",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter-00-introduction.html#structure-and-learning-path",
    "href": "chapter-00-introduction.html#structure-and-learning-path",
    "title": "1  Introduction",
    "section": "1.7 Structure and Learning Path",
    "text": "1.7 Structure and Learning Path\nThe book proceeds from foundations to applications:\n\nChapters 1–3 establish conceptual groundwork, mixed-type data challenges, and data preparation\nChapters 4–6 focus on association measures and network representations\nChapters 7–9 introduce interactive visual analytics\nChapters 10–18 present three families of advanced methods (trees, interpretable ML, AutoML)\nChapters 19–21 present extended applied case studies\n\nReaders can follow a linear path or jump to chapters matching their immediate needs. Code examples and exercises throughout encourage hands-on practice.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter-00-introduction.html#computational-tools",
    "href": "chapter-00-introduction.html#computational-tools",
    "title": "1  Introduction",
    "section": "1.8 Computational Tools",
    "text": "1.8 Computational Tools\nExamples use R, chosen for its rich ecosystem of statistical graphics and modeling packages. Key packages include:\n\n{ggplot2} and {plotly} for visualization\n{rpart} and {partykit} for tree-based methods\n{iml} for interpretable ML\n{h2o} for AutoML\n{corrr}, {energy}, {minerva} for association measures\n{igraph} and {ggraph} for network visualization\n{shiny} for interactive applications\n\nAll code is provided in reproducible format, and datasets are publicly available.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter-00-introduction.html#looking-ahead",
    "href": "chapter-00-introduction.html#looking-ahead",
    "title": "1  Introduction",
    "section": "1.9 Looking Ahead",
    "text": "1.9 Looking Ahead\nThe chapters that follow present a coherent toolkit for advanced descriptive analysis. While methods vary, the underlying goal remains constant: to help you see more deeply into your data, communicate findings clearly, and make better-informed decisions.\nDescriptive analysis is both art and science—requiring statistical rigor, visual judgment, and domain knowledge. We hope this book equips you with methods and perspectives that enhance all three.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter-01-beyond-basic-descriptives.html",
    "href": "chapter-01-beyond-basic-descriptives.html",
    "title": "2  Beyond Basic Descriptive Statistics",
    "section": "",
    "text": "2.1 Why “Basic” Summaries Are Not Enough\nA mean, a median, and a standard deviation can be computed in seconds, but they rarely tell the whole story. Two datasets can share identical means and variances while having radically different shapes, outlier structures, or subgroup patterns. In high‑dimensional datasets, univariate summaries also conceal interaction and conditional relationships that often matter more than any marginal distribution.\nBasic descriptive statistics are necessary but not sufficient. This chapter expands the descriptive toolbox with methods that remain non‑inferential but offer far richer insight into data structure. The goal is to answer more nuanced questions:",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Beyond Basic Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter-01-beyond-basic-descriptives.html#why-basic-summaries-are-not-enough",
    "href": "chapter-01-beyond-basic-descriptives.html#why-basic-summaries-are-not-enough",
    "title": "2  Beyond Basic Descriptive Statistics",
    "section": "",
    "text": "Where are the mass and tails of the distribution?\nAre there subpopulations with distinct profiles?\nAre relationships nonlinear, heterogeneous, or conditional?\nWhich variables are stable versus volatile across subgroups?",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Beyond Basic Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter-01-beyond-basic-descriptives.html#distributional-shape-beyond-mean-and-variance",
    "href": "chapter-01-beyond-basic-descriptives.html#distributional-shape-beyond-mean-and-variance",
    "title": "2  Beyond Basic Descriptive Statistics",
    "section": "2.2 Distributional Shape: Beyond Mean and Variance",
    "text": "2.2 Distributional Shape: Beyond Mean and Variance\n\n2.2.1 Quantiles and Tail Behavior\nQuantiles describe where the data live, not just how they average out. In applied settings, percentiles often carry more operational meaning than averages. The 90th percentile of response time, income, or waiting time is usually more informative than a mean.\nCommon descriptive quantiles:\n\nMedian (\\(Q_{0.5}\\)): robust center\nInterquartile range (IQR): spread of the middle 50%\nTail quantiles: \\(Q_{0.9}\\), \\(Q_{0.95}\\), \\(Q_{0.99}\\) for risk or extreme behavior\n\nThese are especially important in skewed or heavy‑tailed distributions where the mean can be misleading.\n\n\n2.2.2 Skewness, Kurtosis, and Robust Alternatives\nSkewness and kurtosis summarize asymmetry and tail heaviness, but they are sensitive to outliers. In descriptive work, robust measures often provide more stable diagnostics:\n\nMedian absolute deviation (MAD) as a scale measure\nRobust z‑scores using median and MAD instead of mean and standard deviation (SD)\nQuantile ratios (e.g., \\(Q_{0.9}/Q_{0.5}\\)) for skewness proxies\n\nThese measures preserve descriptive intent while reducing sensitivity to extreme observations.\n\n\n2.2.3 Density and Empirical Distribution Functions\nHistograms can be misleading due to binning choices. Kernel density estimates (KDEs) and empirical CDFs show shape more faithfully. ECDFs are particularly useful for comparing distributions because they show the full cumulative structure without smoothing.\n\n# Density and ECDF side by side\np1 &lt;- mtcars %&gt;%\n  ggplot(aes(x = mpg)) +\n  geom_density(fill = \"grey80\", color = \"grey20\") +\n  labs(title = \"KDE of mpg\", x = \"mpg\", y = \"Density\")\n\np2 &lt;- mtcars %&gt;%\n  ggplot(aes(x = mpg)) +\n  stat_ecdf(geom = \"step\") +\n  labs(title = \"ECDF of mpg\", x = \"mpg\", y = \"F(x)\")\n\np1 + p2",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Beyond Basic Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter-01-beyond-basic-descriptives.html#multimodality-and-mixtures",
    "href": "chapter-01-beyond-basic-descriptives.html#multimodality-and-mixtures",
    "title": "2  Beyond Basic Descriptive Statistics",
    "section": "2.3 Multimodality and Mixtures",
    "text": "2.3 Multimodality and Mixtures\nA single distribution can conceal multiple regimes. For example, household income often reflects a mixture of wage earners, retirees, and business owners. Multimodality is a descriptive signal of underlying subpopulations. Techniques to detect it include:\n\nKernel density plots with multiple peaks\nBimodality coefficients or dip tests (used descriptively)\nMixture summaries (e.g., fitting a Gaussian mixture model purely for segmentation)\n\nEven without formal modeling, visual inspection and stratified summaries can reveal important mixtures.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Beyond Basic Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter-01-beyond-basic-descriptives.html#bivariate-and-conditional-descriptives",
    "href": "chapter-01-beyond-basic-descriptives.html#bivariate-and-conditional-descriptives",
    "title": "2  Beyond Basic Descriptive Statistics",
    "section": "2.4 Bivariate and Conditional Descriptives",
    "text": "2.4 Bivariate and Conditional Descriptives\n\n2.4.1 Conditional Means and Quantiles\nUnivariate summaries hide conditional variation. A variable may have a stable mean overall but vary dramatically across categories. Conditional statistics are simple to compute and often reveal key structure:\n\n\\(E(Y\\mid X)\\): mean outcomes by group\n\\(Q_{0.5}(Y\\mid X)\\): median outcomes by group\n\\(\\text{IQR}(Y\\mid X)\\): spread by group\n\nThese summaries can be visualized using grouped boxplots, violin plots, or ridgeline plots.\n\n\n2.4.2 Nonlinear Relationships\nScatterplots with smoothers (e.g., LOESS) often reveal nonlinear trends that correlations miss. A zero correlation does not imply “no relationship”; it may reflect a U‑shape, threshold effect, or segmented pattern.\nA useful descriptive strategy is to compute binned summaries: divide a continuous predictor into quantile bins and summarize the response within each bin. This provides a simple approximation of conditional structure without invoking a full model.\n\n# Binned summaries to reveal nonlinear structure\nmtcars %&gt;%\n  mutate(wt_bin = ntile(wt, 5)) %&gt;%\n  group_by(wt_bin) %&gt;%\n  summarise(\n    mpg_median = median(mpg, na.rm = TRUE),\n    mpg_iqr = IQR(mpg, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  gt() %&gt;%\n  fmt_number(columns = c(mpg_median, mpg_iqr), decimals = 2)\n\n\n\n\n\n\n\nwt_bin\nmpg_median\nmpg_iqr\n\n\n\n\n1\n30.40\n4.75\n\n\n2\n21.00\n1.10\n\n\n3\n18.95\n2.82\n\n\n4\n15.35\n1.80\n\n\n5\n14.00\n4.85\n\n\n\n\n\n\nmtcars %&gt;%\n  mutate(wt_bin = ntile(wt, 5)) %&gt;%\n  group_by(wt_bin) %&gt;%\n  summarise(mpg_median = median(mpg), .groups = \"drop\") %&gt;%\n  ggplot(aes(x = wt_bin, y = mpg_median)) +\n  geom_line() +\n  geom_point() +\n  labs(x = \"Weight bin (quintiles)\", y = \"Median mpg\")\n\n\n\n\n\n\n\n\n\n\n2.4.3 Association Heterogeneity\nAssociations can differ across subgroups. An overall correlation might hide a strong relationship within a subgroup or even mask a reversal (Simpson’s paradox). Descriptive analysis should therefore report stratified associations when meaningful groupings exist.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Beyond Basic Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter-01-beyond-basic-descriptives.html#outliers-extremes-and-influence",
    "href": "chapter-01-beyond-basic-descriptives.html#outliers-extremes-and-influence",
    "title": "2  Beyond Basic Descriptive Statistics",
    "section": "2.5 Outliers, Extremes, and Influence",
    "text": "2.5 Outliers, Extremes, and Influence\nOutliers are not always errors. They often carry substantive meaning: high‑risk patients, exceptional transactions, or rare events. Descriptive analysis should treat outliers as signals first, and errors second.\nKey descriptive checks include:\n\nTail inspection: list the largest/smallest observations\nInfluence screening: compare summaries with and without extreme values\nRobust summaries: medians, trimmed means, and MAD\n\nA practical workflow is to compute both standard and robust summaries side by side. Large divergence is a flag that distributional extremes matter.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Beyond Basic Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter-01-beyond-basic-descriptives.html#missing-data-as-descriptive-information",
    "href": "chapter-01-beyond-basic-descriptives.html#missing-data-as-descriptive-information",
    "title": "2  Beyond Basic Descriptive Statistics",
    "section": "2.6 Missing Data as Descriptive Information",
    "text": "2.6 Missing Data as Descriptive Information\nMissingness is itself informative. The pattern of missing data can reveal survey fatigue, data collection issues, or systematic exclusion of certain groups.\nDescriptive checks include:\n\nMissingness rates by variable\nMissingness by subgroup (e.g., higher nonresponse among certain demographics)\nCo‑missingness patterns (variables missing together)\n\nUnderstanding missingness patterns is a prerequisite for credible descriptive analysis because it reveals which parts of the data are under‑observed or biased.\n\n# Simple missingness profile\nairquality %&gt;%\n  summarise(across(everything(), ~ mean(is.na(.x)))) %&gt;%\n  pivot_longer(everything(), names_to = \"variable\", values_to = \"missing_rate\") %&gt;%\n  arrange(desc(missing_rate)) %&gt;%\n  gt() %&gt;%\n  fmt_percent(columns = missing_rate, decimals = 1)\n\n\n\n\n\n\n\nvariable\nmissing_rate\n\n\n\n\nOzone\n24.2%\n\n\nSolar.R\n4.6%\n\n\nWind\n0.0%\n\n\nTemp\n0.0%\n\n\nMonth\n0.0%\n\n\nDay\n0.0%\n\n\n\n\n\n\n# Co-missingness count matrix\nmiss_mat &lt;- airquality %&gt;% mutate(across(everything(), is.na))\nco_miss &lt;- t(as.matrix(miss_mat)) %*% as.matrix(miss_mat)\nco_miss %&gt;%\n  as.data.frame() %&gt;%\n  tibble::rownames_to_column(var = \"variable\") %&gt;%\n  gt()\n\n\n\n\n\n\n\nvariable\nOzone\nSolar.R\nWind\nTemp\nMonth\nDay\n\n\n\n\nOzone\n37\n2\n0\n0\n0\n0\n\n\nSolar.R\n2\n7\n0\n0\n0\n0\n\n\nWind\n0\n0\n0\n0\n0\n0\n\n\nTemp\n0\n0\n0\n0\n0\n0\n\n\nMonth\n0\n0\n0\n0\n0\n0\n\n\nDay\n0\n0\n0\n0\n0\n0",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Beyond Basic Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter-01-beyond-basic-descriptives.html#scaling-standardization-and-comparability",
    "href": "chapter-01-beyond-basic-descriptives.html#scaling-standardization-and-comparability",
    "title": "2  Beyond Basic Descriptive Statistics",
    "section": "2.7 Scaling, Standardization, and Comparability",
    "text": "2.7 Scaling, Standardization, and Comparability\nWhen comparing variables on different scales, raw summaries can mislead. Standardization puts variables on a common metric:\n\\[\nZ = \\frac{X - \\mu}{\\sigma}\n\\]\nHowever, standardization is not always desirable. For skewed or heavy‑tailed distributions, robust scaling using medians and MAD can be more appropriate:\n\\[\nZ_{\\text{robust}} = \\frac{X - \\text{median}(X)}{\\text{MAD}(X)}\n\\]\nStandardization is especially useful when building profiles of observations across many variables, a topic revisited in later chapters on clustering and tree‑based methods.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Beyond Basic Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter-01-beyond-basic-descriptives.html#multivariate-profiles-and-descriptive-models",
    "href": "chapter-01-beyond-basic-descriptives.html#multivariate-profiles-and-descriptive-models",
    "title": "2  Beyond Basic Descriptive Statistics",
    "section": "2.8 Multivariate Profiles and “Descriptive Models”",
    "text": "2.8 Multivariate Profiles and “Descriptive Models”\nAs dimensionality increases, summaries must become multivariate. Two simple but powerful tools are:\n\nProfile tables: compare multiple variables across key subgroups (e.g., demographic segments)\nComposite indices: average or weighted sums of standardized variables to create a high‑level descriptive score\n\nComposite indices are not causal models; they are descriptive constructs that summarize a multidimensional concept (e.g., socioeconomic status, health risk, engagement intensity). Transparency in construction is essential for interpretability.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Beyond Basic Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter-01-beyond-basic-descriptives.html#visual-descriptives-that-scale",
    "href": "chapter-01-beyond-basic-descriptives.html#visual-descriptives-that-scale",
    "title": "2  Beyond Basic Descriptive Statistics",
    "section": "2.9 Visual Descriptives That Scale",
    "text": "2.9 Visual Descriptives That Scale\nCertain visual tools provide richer descriptive information than conventional charts:\n\nViolin plots: show full distribution and density\nBoxen plots: emphasize tails across many observations\nRidge plots: compare distributions across many groups\nHeatmaps: visualize large tables of summary statistics\nECDFs: compare distributions without binning\n\nThese graphics remain descriptive but give a more faithful sense of distributional complexity and subgroup structure.\n\n# Heatmap of median summary statistics across groups\nmtcars %&gt;%\n  select(mpg, hp, wt, cyl) %&gt;%\n  group_by(cyl) %&gt;%\n  summarise(across(c(mpg, hp, wt), median), .groups = \"drop\") %&gt;%\n  pivot_longer(-cyl, names_to = \"variable\", values_to = \"median\") %&gt;%\n  ggplot(aes(x = factor(cyl), y = variable, fill = median)) +\n  geom_tile() +\n  scale_fill_viridis_c() +\n  labs(x = \"Cylinders\", y = \"Variable\", fill = \"Median\")",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Beyond Basic Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter-01-beyond-basic-descriptives.html#a-practical-workflow",
    "href": "chapter-01-beyond-basic-descriptives.html#a-practical-workflow",
    "title": "2  Beyond Basic Descriptive Statistics",
    "section": "2.10 A Practical Workflow",
    "text": "2.10 A Practical Workflow\nA robust descriptive workflow often follows this sequence:\n\nUnivariate inspection: quantiles, density plots, robust summaries\nMissingness mapping: rates, patterns, co‑missingness\nBivariate exploration: conditional summaries, scatterplots with smoothers\nStratified checks: subgroup comparisons, heterogeneity in associations\nMultivariate summaries: profiles and composite indices\n\nThis workflow remains entirely descriptive while systematically uncovering structure that basic summary tables would miss.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Beyond Basic Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter-01-beyond-basic-descriptives.html#example-a-small-r-template",
    "href": "chapter-01-beyond-basic-descriptives.html#example-a-small-r-template",
    "title": "2  Beyond Basic Descriptive Statistics",
    "section": "2.11 Example: A Small R Template",
    "text": "2.11 Example: A Small R Template\nThe following minimal template illustrates how to go beyond means and SDs with a few descriptive enhancements:\n\n# Robust summary for a numeric variable (tidy output)\nsummary_stats &lt;- function(x) {\n  tibble(\n    mean = mean(x, na.rm = TRUE),\n    median = median(x, na.rm = TRUE),\n    sd = sd(x, na.rm = TRUE),\n    mad = mad(x, na.rm = TRUE),\n    q10 = quantile(x, 0.10, na.rm = TRUE),\n    q90 = quantile(x, 0.90, na.rm = TRUE)\n  )\n}\n\n# Conditional summaries by group\nmtcars %&gt;%\n  group_by(cyl) %&gt;%\n  summarise(summary_stats(mpg), .groups = \"drop\") %&gt;%\n  gt() %&gt;%\n  fmt_number(columns = c(mean, median, sd, mad, q10, q90), decimals = 2)\n\n\n\n\n\n\n\ncyl\nmean\nmedian\nsd\nmad\nq10\nq90\n\n\n\n\n4\n26.66\n26.00\n4.51\n6.52\n21.50\n32.40\n\n\n6\n19.74\n19.70\n1.45\n1.93\n17.98\n21.16\n\n\n8\n15.10\n15.20\n2.56\n1.56\n11.27\n18.28\n\n\n\n\n\n\n\nThe aim is not sophistication but discipline: always inspect robust, conditional, and distributional features before moving to advanced methods.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Beyond Basic Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter-01-beyond-basic-descriptives.html#looking-ahead",
    "href": "chapter-01-beyond-basic-descriptives.html#looking-ahead",
    "title": "2  Beyond Basic Descriptive Statistics",
    "section": "2.12 Looking Ahead",
    "text": "2.12 Looking Ahead\nThis chapter expands the descriptive mindset beyond simple averages. The next chapters formalize these ideas for mixed data types and systematic association measures. Once variable types are correctly handled, we can build type‑aware association matrices, visualize them as networks, and scale descriptive analysis to hundreds of variables.\nThe key lesson is simple: descriptive analysis improves when we stop summarizing variables in isolation and start describing structure.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Beyond Basic Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter-03-data-preparation.html",
    "href": "chapter-03-data-preparation.html",
    "title": "4  Data Preparation for Descriptive Analysis",
    "section": "",
    "text": "4.1 3.1 Understanding Variables and Metadata\nDescriptive analysis is only as reliable as the data that feed it. Before measuring associations or building models, analysts must understand variable definitions, clean inconsistencies, and construct analysis-ready tables. This chapter introduces practical workflows for preparing messy real-world data so that later descriptive methods are interpretable, reproducible, and trustworthy.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation for Descriptive Analysis</span>"
    ]
  },
  {
    "objectID": "chapter-03-data-preparation.html#understanding-variables-and-metadata",
    "href": "chapter-03-data-preparation.html#understanding-variables-and-metadata",
    "title": "4  Data Preparation for Descriptive Analysis",
    "section": "",
    "text": "4.1.1 Goals\n\nClarify variable meaning, units, and coding schemes\nDetect ambiguous or inconsistent definitions across sources\nDocument assumptions and transformations for reproducibility\n\n\n\n4.1.2 Topics\n\nData dictionaries and codebooks\nUnits, scales, and measurement error\nCategorical coding (ordered vs. nominal)\nTime and panel identifiers\nProvenance and data lineage",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation for Descriptive Analysis</span>"
    ]
  },
  {
    "objectID": "chapter-03-data-preparation.html#cleaning-harmonization-and-quality-checks",
    "href": "chapter-03-data-preparation.html#cleaning-harmonization-and-quality-checks",
    "title": "4  Data Preparation for Descriptive Analysis",
    "section": "4.2 3.2 Cleaning, Harmonization, and Quality Checks",
    "text": "4.2 3.2 Cleaning, Harmonization, and Quality Checks\n\n4.2.1 Goals\n\nIdentify errors, missingness, and outliers\nHarmonize variables across files or time periods\nApply minimal, transparent corrections\n\n\n\n4.2.2 Topics\n\nMissing data patterns and reporting\nRange checks and logical constraints\nDuplicate records and entity resolution\nStandardizing categories and labels\nDealing with inconsistent time formats",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation for Descriptive Analysis</span>"
    ]
  },
  {
    "objectID": "chapter-03-data-preparation.html#feature-construction-for-descriptive-insight",
    "href": "chapter-03-data-preparation.html#feature-construction-for-descriptive-insight",
    "title": "4  Data Preparation for Descriptive Analysis",
    "section": "4.3 3.3 Feature Construction for Descriptive Insight",
    "text": "4.3 3.3 Feature Construction for Descriptive Insight\n\n4.3.1 Goals\n\nCreate interpretable derived variables\nEncode variables for mixed-type association measures\nPreserve interpretability while enabling analysis\n\n\n\n4.3.2 Topics\n\nBinning and discretization (with justification)\nRatios, rates, and per-capita measures\nIndex construction and composite measures\nNormalization and scaling choices\nAudit trails for transformations",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation for Descriptive Analysis</span>"
    ]
  },
  {
    "objectID": "chapter-03-data-preparation.html#reproducible-preparation-pipelines",
    "href": "chapter-03-data-preparation.html#reproducible-preparation-pipelines",
    "title": "4  Data Preparation for Descriptive Analysis",
    "section": "4.4 3.4 Reproducible Preparation Pipelines",
    "text": "4.4 3.4 Reproducible Preparation Pipelines\n\n4.4.1 Goals\n\nMake preparation steps transparent and repeatable\nSeparate raw, intermediate, and analysis-ready data\nFacilitate collaboration and review\n\n\n\n4.4.2 Topics\n\nScripted pipelines vs. manual edits\nVersioning datasets and metadata\nValidation checks as code\nSummary reports for prepared datasets",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation for Descriptive Analysis</span>"
    ]
  },
  {
    "objectID": "chapter-03-data-preparation.html#applied-example-preparing-a-mixed-type-dataset",
    "href": "chapter-03-data-preparation.html#applied-example-preparing-a-mixed-type-dataset",
    "title": "4  Data Preparation for Descriptive Analysis",
    "section": "4.5 3.5 Applied Example: Preparing a Mixed-Type Dataset",
    "text": "4.5 3.5 Applied Example: Preparing a Mixed-Type Dataset\nA brief walkthrough demonstrates how a raw survey dataset is transformed into an analysis-ready table with clear variable definitions, cleaned categories, and documented transformations. The focus is on traceability: every transformation is explained, and the resulting dataset is ready for association analysis and visual exploration.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation for Descriptive Analysis</span>"
    ]
  }
]